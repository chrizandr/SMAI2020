\begin{frame}
\section{}
About Backpropagation Algorithm:
\begin{enumerate}[label=(\Alph*)]
\item When the algorithm  is allowed to run for many (say $\infty$) iterations, the loss becomes zero.
\item When the algorithm  is allowed to run for many (say $\infty$) iterations, the network will overfit.
\item When the algorithm  is allowed to run for many (say $\infty$) iterations, we will reach a local minima.   % Ans
\item When the algorithm  is allowed to run for many (say $\infty$) iterations, we will reach the same local minima, irrespective of the initialization
\item All the above are true.  % None
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
About Backpropagation Algorithm:

Which may be a really {\bf bad} termination crieria
\begin{enumerate}[label=(\Alph*)]
\item When no major change in loss, end.
\item When all gradients are near zero, end.
\item When learning rate is near zero, end.   % Ans
\item When loss is near zero, end.
\item All the above are terrible termination crieria.   % None
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
Consider an MLP getting used for a three class classification problem.

Output layer has three neurons and we use a cross entropy loss.
\begin{enumerate}[label=(\Alph*)]
\item If the accuracy of the training data is 100\%, it implies that the loss might have been zero.
\item If the loss is zero, implies that the MLP as a classifier has 100\% accuracy on the training data.    % Ans
\item If the loss is zero, implies that the MLP as a classifier has 100\% accuracy on the test data.
\item If the accuracy of the test data is 100\%, it implies that the loss computed on the training data might  have been zero.
\item None of the above.  % None
\end{enumerate}
\end{frame}


\begin{frame}
\section{}
Make the necessary minimal changes  (if any required) and rewrite as true sentences in the space provided. Avoid changing the words in bold.

{\bf An MLP  has no activation in the  output. It has  sigmoid activity in all the hidden layers.  }{\em It can not be used to output negative values} {\bf because }{\em  sigmoid outputs in [0,1] (no negative)}
% FIB
\end{frame}

\begin{frame}
\section{}
Make the necessary minimal changes  (if any required) and rewrite as true sentences in the space provided. Avoid changing the words in bold.

{\bf Backpropagation algorithm } {\em can be understood as an iterative optimization algorithm.}

% FIB
\end{frame}
