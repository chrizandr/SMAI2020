\begin{frame}
\section{}
We know that the VC dimension of a set of lines in 2D is 3. What is the VC dimension of a set of planes in 3D?
\begin{enumerate}[label=(\Alph*)]
\item 3+1 = 4   % Ans
\item 2+2 = 2
\item $2 \times \frac{3}{4} = 6$
\item Remains the same. i.e., 3
\item None of the above   % None
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
We know that $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$. What is the derivative of $tanh(x)$
\begin{enumerate}[label=(\Alph*)]
\item $1+\tanh(x)$
\item $1- \tanh^2(x)$   % Ans
\item $\tanh(x)(1-\tanh(x))$
\item $1+\tanh^2(x)$
\item None of the above   % None
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
An MLP has two inputs, two hidden layers of 3 neurons each and an output of two neurons. All the neurons have biases. The number of weights (or learnable parameters) is:

\begin{enumerate}[label=(\Alph*)]
\item 24
\item 21
\item 29    % Ans
\item 37
\item None of the above  % None
\end{enumerate}
\end{frame}


\begin{frame}
\section{}
Make the necessary minimal changes  (if any required) and rewrite as true sentences in the space provided. Avoid changing the words in bold.

{\em A Single Layer Perceptron} {\bf can solve ExOR problem}.
% FIB
\end{frame}

\begin{frame}
\section{}
Make the necessary minimal changes  (if any required) and rewrite as true sentences in the space provided. Avoid changing the words in bold.

{\bf Backpropagation algorithm} {\em can guarantee (always find) the optimal solution/weights} {\bf for a Multilayer Perceptron}.
% FIB
\end{frame}
