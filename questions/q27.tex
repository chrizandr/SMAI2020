\begin{frame}
\section{}
Consider the ReLU activation function for a neuron.

\begin{enumerate}[label=(\Alph*)]
\item Output is always non-negative.    % Ans
\item Output is always positive
\item output is either one or zero.
\item Output is same as input.
\item None of the above.  % None
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
Consider the ReLU activation function for a neuron. Derivative of the ReLu function:
\begin{enumerate}[label=(\Alph*)]
\item continuous
\item differentiable
\item is Constant throughout
\item can take two values.    % Ans
\item can never be negative   % Ans
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
Consider an MLP with 2 inputs, 3 neurons in hidden and one output. Hidden neurons and output neuron uses ReLU Activation.

Let the input be $x_1$ and $x_2$ and output be $y$. We train this with MSE loss.
\begin{enumerate}[label=(\Alph*)]
\item If $x_1$, $x_2$ are negative, and $y$ is positive for all the samples, this network can not be used for effective problem solving.
\item If $x_1$, $x_2$ are positive, and $y$ is negative for all the samples, this network can not be used for effective problem solving.    % Ans
\item This network can be effectively used irrespective of whether input or output is negative.
\item This network can not be useful if either input or output is negative.
\item None of the above.  % None
\end{enumerate}
\end{frame}


\begin{frame}
\section{}
Make the necessary minimal changes  (if any required) and rewrite as true sentences in the space provided. Avoid changing the words in bold.

{\bf Consider a deep neural network with ReLU activations.} {\em Since the gradient is same as input (which can be very large quantity), there is a chance of vanishing gradient problem.}
% FIB
\end{frame}

\begin{frame}
\section{}
Make the necessary minimal changes  (if any required) and rewrite as true sentences in the space provided. Avoid changing the words in bold.

{\bf For leaky ReLu, gradients are} {\em either positive or negative}.
% FIB
\end{frame}
