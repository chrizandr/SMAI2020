\begin{frame}
\section{}
Consider a typical two class classification problem. We have a labelled set of 1000 samples. (say $N=1000, d=2$).

We train a classifier iterative (say using GD) and minimize a loss function (eg. a mean square error loss).

We use 80\% data for training (i.e., 800) and rest 20\% (i.e., 200) for testing.
\begin{enumerate}
\item To train the model, We iterate until the loss on the training data becomes zero.
\item If we allow to continue for enough time the the loss on training data will eventually become zero.
\item If at any point of time, the loss on training data is zero, then the loss on test data will also be zero.
\item If both training and test loss are zero, then we are sure that the algorithm has overfit.
\item None of the above.   % None       % Ans
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
Consider a two class classification problem. We have a labelled set of 1000 samples. (say $N=1000, d=2)$.

We train a classifier iterative (say GD) and minimize a loss function (eg. a mean square error loss) by using all the samples.


\underline {In each iteration,} we use  a random
80\% of the total data as training (i.e., 800) and rest 20\% (i.e., 200) for testing.
\begin{enumerate}
\item This is perfectly fine way of training the ML solution.
\item This iterative algorithm will not converge.
\item Since the test data is changed on a regular basis, the solution will generalize well.
\item Since the training data is changing in every iteration (or regular basis), the loss will not come down.
\item None of the above     % None       % Ans
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
Consider a typical two class classification problem. We have a labelled set of 1000 samples. (say $N=1000, d=2$).

We train a classifier iterative (say using a GD) and minimize a loss function (eg. a mean square error loss).

We use 80\% data for training (i.e., 800) and rest 20\% (i.e., 200) for testing.

During the iteration $k$,  loss on the training data and Test data are $L_{Tr}^k$ and $L_{Te}^k$.  Let the accuracy of the classifier at iteration $i$ be $\eta_{Tr}^i$ and $\eta_{Te}^i$.
\begin{enumerate}
\item If $L^k_{Tr} \gg L^l_{Tr}$, then $l > k$.     % Ans
\item If $L^k_{Tr} \gg L^l_{Tr}$, then $\eta_{Tr}^k > \eta_{Tr}^l$ (strictly greater).
\item If $L^k_{Tr} > L^l_{Tr}$, then If $L^k_{Te} > L^l_{Te}$.
\item If $L^k_{Tr} > L^l_{Tr}$, then If $L^k_{Te} < L^l_{Te}$.
\item None of the above.   % None
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
In the context of regularization:
\begin{enumerate}
\item $L_\infty$ regularization leads to sparse solution.
\item $L_2$ regularization leads to sparse solution.
\item $L_1$ regularization leads to sparse solution.        % Ans
\item $L_0$ regularization leads to sparse solution.        % Ans
\item Any regularization will lead to sparse solution.
\end{enumerate}
\end{frame}

\begin{frame}
\section{}
In the context of supervised machine learning,
\begin{enumerate}
\item Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points.   % Ans
\item Occam's Razor says: Suppose there exist two explanations for an occurrence. In this case the one that requires the smallest number of assumptions is usually correct.     % Ans
\item Supervised learning is all about overfitting to the given data.
\item Regularization decrease the chance of overfitting.    % Ans
\item None of the above.        % None
\end{enumerate}
\end{frame}
