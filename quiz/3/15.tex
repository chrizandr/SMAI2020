
\begin{frame}
\section{}
Consider a deep MLP and shallow MLP. Both gives the same loss and accuracy on the training data trained with the same number of samples.
\begin{enumerate}[label=(\Alph*)]
\item We prefer deep MLP (since deep neural networks are the best as of now)
\item We prefer shallow MLP   % Ans
\item Both are equally good.
\item Both neural networks then represent the same function. (since the loss is equal on both)
\item None of the above.  % None

\end{enumerate}

\end{frame}


\begin{frame}
\section{}
Consider a deep MLP and shallow MLP. Both are trained with the same number of samples.

\begin{enumerate}[label=(\Alph*)]
\item It is highly likely that Deep MLP will have lower training error. (since deeper the powerful!)    % Ans
\item It is highly likely that the shallow MLP will have lower training error. (since Occam's Razor says so)
\item If the number of training samples is small, Deep MLP is going to overfit.   % Ans
\item If the number of training samples is small, Shallow MLP is going to overfit.
\item None of the above    % None
\end{enumerate}
\end{frame}
