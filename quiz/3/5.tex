\begin{frame}
\section{}
Consider an MLP with two input, one output and one hidden layer with two neurons. No bias. All weights are 1.0.

Hidden neurons have ReLu Activation and output has tanh activation.

Find the output of this MLP for an input of $[1, -2]^T$
% FIB

\end{frame}


\begin{frame}
\section{}
Consider an MLP with two input, one output and one hidden layer with two neurons. No bias. All weights are 2.0.

Hidden neurons have ReLu Activation and output has tanh activation.

Find the output of this MLP for an input of $[1, -2]^T$

% FIB
\end{frame}


\begin{frame}
\section{}
Consider an MLP with two input, one output and one hidden layer with two neurons. No bias. All weights are -1.0.

Hidden neurons have ReLu Activation and output has tanh activation.

Find the output of this MLP for an input of $[1, -2]^T$

% FIB
\end{frame}


\begin{frame}
\section{}
Consider an MLP with two input, one output and one hidden layer with two neurons. No bias. All weights are 1.0.

Hidden neurons have ReLu Activation and output has tanh activation.

Find the output of this MLP for an input of $[-2, -3]^T$
% FIB
\end{frame}


\begin{frame}
\section{}
Consider an MLP with two input, one output and one hidden layer with two neurons. No bias. All weights are unity.

Hidden neurons have ReLu Activation and output has tanh activation.

Find the output of this MLP for an input of $[2, -3]^T$
% FIB
\end{frame}
