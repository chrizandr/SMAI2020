\begin{frame}
\section{}
Consider an MLP with two inputs, three hidden neurons and one output neurons. Hidden neurons and output neurons have sigmoid activation. There is no bias. Output neuron has a MSE loss.

Consider a sample $([5,5]^T,0.7)$ i.e., $x = [5,5]^T$ and $y=0.7$. We would like to update all the weights based on the gradient of the loss $({\cal L})$. Assume that $w_{ij}^{[k]}$ connects ith neuron of layer k with jth neuron of layer k+1. Thus weights between input and hidden layer are $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}, w_{13}^{[1]}, w_{23}^{[1]}$ and those between hidden layer and output layer are $w_{11}^{[2]}, w_{21}^{[2]}, w_{31}^{[2]}$
All weights are initialized with unity.
Find the numerical value of $\frac{\partial \cal L}{\partial w_{11}^{[1]}}$. Answer upto 4 decimal places.

% FIB

\end{frame}

\begin{frame}
\section{}
Consider an MLP with two inputs, three hidden neurons and one output neurons. Hidden neurons and output neurons have sigmoid activation. There is no bias. Output neuron has a MSE loss.

Consider a sample $([5,5]^T,0.7)$ i.e., $x = [5,5]^T$ and $y=0.7$. We would like to update all the weights based on the gradient of the loss $({\cal L})$. Assume that $w_{ij}^{[k]}$ connects ith neuron of layer k with jth neuron of layer k+1. Thus weights between input and hidden layer are $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}, w_{13}^{[1]}, w_{23}^{[1]}$ and those between hidden layer and output layer are $w_{11}^{[2]}, w_{21}^{[2]}, w_{31}^{[2]}$
All weights are initialized with unity.

Find the numerical value of $\frac{\partial \cal L}{\partial w_{12}^{[1]}}$. Answer upto 4 decimal places.

% FIB

\end{frame}

\begin{frame}
\section{}
Consider an MLP with two inputs, three hidden neurons and one output neurons. Hidden neurons and output neurons have sigmoid activation. There is no bias. Output neuron has a MSE loss.

Consider a sample $([5,5]^T,0.7)$ i.e., $x = [5,5]^T$ and $y=0.7$. We would like to update all the weights based on the gradient of the loss $({\cal L})$. Assume that $w_{ij}^{[k]}$ connects ith neuron of layer k with jth neuron of layer k+1. Thus weights between input and hidden layer are $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}, w_{13}^{[1]}, w_{23}^{[1]}$ and those between hidden layer and output layer are $w_{11}^{[2]}, w_{21}^{[2]}, w_{31}^{[2]}$
All weights are initialized with unity.

Find the numerical value of $\frac{\partial \cal L}{\partial w_{13}^{[1]}}$. Answer upto 4 decimal places.

% FIB

\end{frame}

\begin{frame}
\section{}
Consider an MLP with two inputs, three hidden neurons and one output neurons. Hidden neurons and output neurons have sigmoid activation. There is no bias. Output neuron has a MSE loss.

Consider a sample $([5,5]^T,0.7)$ i.e., $x = [5,5]^T$ and $y=0.7$. We would like to update all the weights based on the gradient of the loss $({\cal L})$. Assume that $w_{ij}^{[k]}$ connects ith neuron of layer k with jth neuron of layer k+1. Thus weights between input and hidden layer are $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}, w_{13}^{[1]}, w_{23}^{[1]}$ and those between hidden layer and output layer are $w_{11}^{[2]}, w_{21}^{[2]}, w_{31}^{[2]}$
All weights are initialized with unity.

Find the numerical value of $\frac{\partial \cal L}{\partial w_{23}^{[1]}}$. Answer upto 4 decimal places.


% FIB

\end{frame}

\begin{frame}
\section{}
Consider an MLP with two inputs, three hidden neurons and one output neurons. Hidden neurons and output neurons have sigmoid activation. There is no bias. Output neuron has a MSE loss.

Consider a sample $([5,5]^T,0.7)$ i.e., $x = [5,5]^T$ and $y=0.7$. We would like to update all the weights based on the gradient of the loss $({\cal L})$. Assume that $w_{ij}^{[k]}$ connects ith neuron of layer k with jth neuron of layer k+1. Thus weights between input and hidden layer are $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}, w_{13}^{[1]}, w_{23}^{[1]}$ and those between hidden layer and output layer are $w_{11}^{[2]}, w_{21}^{[2]}, w_{31}^{[2]}$
All weights are initialized with unity.

Find the numerical value of $\frac{\partial \cal L}{\partial w_{11}^{[2]}}$. Answer upto 4 decimal places.

% FIB

\end{frame}

\begin{frame}
\section{}
Consider an MLP with two inputs, three hidden neurons and one output neurons. Hidden neurons and output neurons have sigmoid activation. There is no bias. Output neuron has a MSE loss.

Consider a sample $([5,5]^T,0.7)$ i.e., $x = [5,5]^T$ and $y=0.7$. We would like to update all the weights based on the gradient of the loss $({\cal L})$. Assume that $w_{ij}^{[k]}$ connects ith neuron of layer k with jth neuron of layer k+1. Thus weights between input and hidden layer are $w_{11}^{[1]}, w_{21}^{[1]}, w_{12}^{[1]}, w_{22}^{[1]}, w_{13}^{[1]}, w_{23}^{[1]}$ and those between hidden layer and output layer are $w_{11}^{[2]}, w_{21}^{[2]}, w_{31}^{[2]}$
All weights are initialized with unity.

Find the numerical value of $\frac{\partial \cal L}{\partial w_{31}^{[2]}}$. Answer upto 4 decimal places.

% FIB

\end{frame}
