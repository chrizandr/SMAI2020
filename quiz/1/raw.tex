\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{hyperref}

\setcounter{section}{-1}

\title{SMAI M-2020 Quiz 1}
\author{C V Jawahar}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Test format}
There are 5 questions:
\begin{enumerate}
    \item Randomly sampled questions from past review sessions
    \item Question from lecture videos/notes
    \item Numerical problem from pre-requisite topics
    \item Programming problem
    \item Reasoning/proof problem
\end{enumerate}

\section{Part 1}
\begin{enumerate}
    \item Consider we are using PCA to compress face images using top K eigenvectors and then we do the reconstruction. Then
    \begin{enumerate}
        \item Compression (for face images) is lossy
        \item Compression (for face images) is lossless
        \item Reconstruction will be bad for non-face images (say buildings)
        \item Reconstruction will be good for non-face images (say buildings)
        \item None of these
    \end{enumerate}
    % Ans AC

    \item Consider we are dong PCA to go from $R^2$ data to $R^1$. Consider each point is denoted by $(X_i,Y_i)$. Then in which of these situations will PCA work reasonably well:
    \begin{enumerate}
        \item $Y_i=X_i+10$
        \item $Y_i=X_i+10+\epsilon_i$ where $\epsilon_i\sim N(0,1)$
        \item $X_i^2+Y_i^2 = 10$
        \item $X_i^2+Y_i^2 <= 10$
        \item None of these
    \end{enumerate}
    % Ans: AB (A has linear,B has near-linear relationship which will get captured in PCA)

    \item Consider we have data in $R^2$. Then the linear regression line and the PCA line
    \begin{enumerate}
        \item will always be the same
        \item will never be the same
        \item can sometimes be the same
        \item None of these
    \end{enumerate}
    % Ans: C

    \item We want to do PCA using gradient descent. Then the update rule is
    \begin{enumerate}
        \item $u_{k+1} = \eta\Sigma u_k$
        \item $u_{k+1} = (I+\eta\Sigma)u_k$
        \item $u_{k+1} = (I-\eta\Sigma)u_k$
        \item None of these
    \end{enumerate}
    Assume that $\Sigma$ is the covariance matrix, $\eta$ is the learning rate.
    % Ans: B (The +ve sign comes because we are solving a maximization problem)

    \item PCA solves this problem:
    \[\max_u u^T \Sigma u - \lambda (u^Tu-1)\]
    where $\Sigma$ is the covariance matrix.
    Which of the following are true regarding PCA
    \begin{enumerate}
        \item $\lambda$ is the variance captured by the eigen vector $u$
        \item Sum of variances captured by all eigenvectors is tr($\Sigma$)
        \item If all data points are on a line then at least one of the eigenvalues is 1
        \item If all data points are on a line then at least one of the eigenvalues is 0
    \end{enumerate}
    % Ans: ABD


    \item Let $X=UDV^T$. Then
    \begin{enumerate}
        \item Columns of U are eigenvectors of $X^TX$
        \item Columns of V a re eigenvectors of $X^TX$
        \item Rows of U are eigenvectors of $X^TX$
        \item Rows of V are eigenvectors of $X^TX$
        \item None of these
    \end{enumerate}
    % Ans: B
    % $X^TX = VD^2V^T \implies X^TXV=VD^2 = V\Lambda$

    \item Let $X=UDV^T$. Then
    \begin{enumerate}
        \item Columns of U are eigenvectors of $XX^T$
        \item Columns of V are eigenvectors of $XX^T$
        \item Rows of U are eigenvectors of $XX^T$
        \item Rows of V are eigenvectors of $XX^T$
        \item None of these
    \end{enumerate}
    % Ans: A
    % $XX^T = UD^2U^T \implies XX^TU=UD^2=U\Lambda$

    \item Consider $X$ to be a square matrix of size $n\times n$ and $X=UDV^T$.
    \begin{enumerate}
        \item Both $X^TX$ and $XX^T$ have the same eigenvalues
        \item Both $X^TX$ and $XX^T$ have the same eigenvectors
        \item $X$, $XX^TX$ and $XX^T$ have the same eigenvalues
        \item $D^2$ contains the eigenvalues of $X^TX$ on its diagonal
        \item $D$ contains the eigenvalues of $X^TX$ on its diagonal
        \item None of these
    \end{enumerate}
    % Ans: AD

    \item Consider $X$ to be a square matrix of size $n\times n$ and $X=UDV^T$.
    Then:
    \begin{enumerate}
        \item If rank(X) = $n$, D has all non-zero entries in diagonal.
        \item If rank(X) = $k$, D has $k$ zeros in diagonal
        \item If rank(X) = $k$, D has $n-k$ zeros in diagonal
        \item if rank(X) = $n$ but $\vert A \vert $ is a very small number then, D takes the form $D=diag(d_1,d_2,..,\epsilon)$ where $\epsilon$ is a very small number
        \item None of these
    \end{enumerate}
    % Ans: ACD

    \item Suppose you want to apply PCA to your data $X$
    which is in 2D and you decompose $X$ as $UDV^T$. Then,
    \begin{enumerate}
        \item PCA can be useful if all elements of D are equal
        \item PCA can be useful if all elements of D are not equal
        \item   $D$ is not full-rank if all points in $X$ lie on a straight line
        \item   $V$ is not full-rank if all points in $X$ lie on a straight line
        \item   $D$ is not full-rank if all points in $X$ lie on a circle
        \item None of these
        % Ans: BC
    \end{enumerate}

    \item Given a set of 2D points $X$ on a line that makes 45 degree to the x-axis:
    \[ X = \{ [1,1]^T, [2,2]^T, [3,3]^, [4,4]^T, [5,5]^T \}\]
    We compute the covariance matrix, and its eigen values and eigen vectors. Then:

    \begin{enumerate}
        \item  $\lambda_2 = 0$
        \item  $\lambda_1 = \lambda_2$
        \item  $\lambda_1 = -1$
        \item  $\Sigma$ is singular
        \item none of the above
    \end{enumerate}
    % Ans: AD

      \item Given a set of 2D points $X$ on a line that makes 45 degree to the x-axis:
    \[ X = \{  [-2,2]^T, [-3,3]^, [-4,4]^T, [-5,5]^T [-6,6]^T \}\]
    We compute the covariance matrix, and its eigen values and eigen vectors. Then:

    \begin{enumerate}
        \item  $\lambda_2 = 0$
        \item  $\lambda_1 = \lambda_2$
        \item  $\lambda_1 = -1$
        \item  $\Sigma$ is singular
        \item none of the above
    \end{enumerate}
    % Ans: AD

    \item Given a set of 2D points $X$ on the vertical line $x_1=5$,
     \[ X = \{ [5,1]^T, [5,2]^T, [5,3]^, [5,4]^T, [5,5]^T \}\]
    We now add an additional
    point $[4,3]^T$ to $X$.

    We compute the covariance matrix, and its eigen values and eigen vectors. Then:


    \begin{enumerate}
        \item $\lambda_1 \geq \lambda_2$
        \item ${\bf u}_1$ and ${\bf u}_2$ are nearly orthogonal, but not perfectly orthogonal.
        \item  $\Sigma$ is singular
        \item  $\Sigma$ is diagonal
        \item None of the above.
    \end{enumerate}
    % Ans: AD


    \item Given a set of 2D points $X$ on the vertical line $x_2=5$,
     \[ X = \{ [1,5]^T, [2,5]^T, [3,5]^, [4,5]^T, [5,5]^T \}\]


    We compute the covariance matrix, and its eigen values and eigen vectors. Then:


    \begin{enumerate}
        \item $\lambda_1 \geq \lambda_2$
        \item ${\bf \mu}$ is on the same line.
        \item  $\Sigma$ is singular
        \item  $\Sigma$ is diagonal
        \item None of the above.
    \end{enumerate}
    % Ans: ABCD

   \item Set $X$ has 10 points. 5 of them are on a line that makes 45 degrees with the $x_1$ axis and another 5 from on a line that makes 135 degrees with the $x_1$ axis.

    We compute the covariance matrix, and its eigen values and eigen vectors. Then:

    \begin{enumerate}
        \item $\lambda_1 = \lambda_2 \neq 0$
        \item $\Sigma$ is singular
        \item $\Sigma$ is diagonal
        \item ${\bf \mu}$ is on either of these lines.
        \item None of the above
    \end{enumerate}
    % Ans: E



    \item (use notations  and conventions from the class) Consider the problem of linear regression where we
    minimize the loss
    \[{\cal L}_1 = \frac{1}{N}\sum_{i=1}^N \alpha_i (y_i - {\bf w}^T{\bf x}_i)^2 + \lambda_1 g({\bf w})\] where $g()$ is a regularization term. We also write the loss in matrix form as \[ {\cal L}_2 = \frac{1}{N} [Y-{\bf X}{\bf w}]^TA[Y-{\bf X}{\bf w}] + \lambda_2 g({\bf w}). \]

    \hrule
    If ${\cal L}_1 = {\cal L}_2$ for all $\bf{w}$, then
    \begin{enumerate}
        \item $A$ is a diagonal matrix
        \item $A_{ij} = \alpha_i \cdot \alpha_j $
        \item $A_{ii} = \alpha_i$ else zero
        \item $A_{ii} = \frac{1}{\alpha_i}$ else zero
        \item none of the above
    \end{enumerate}
    % Ans: AC

      \item (use notations  and conventions from the class) Consider the problem of linear regression where we
    minimize the loss
    \[{\cal L}_1 = \frac{1}{N}\sum_{i=1}^N \alpha_i (y_i - {\bf w}^T{\bf x}_i)^2 + \lambda_1 g({\bf w})\] where $g()$ is a regularization term. We also write the loss in matrix form as \[ {\cal L}_2 = \frac{1}{N} [Y-{\bf X}{\bf w}]^TA[Y-{\bf X}{\bf w}] + \lambda_2 g({\bf w}). \]

    \hrule

    If ${\bf A} = I$, $\alpha_i = 1$ for all $i$, and $\lambda_1 = \lambda_2 = 1$, then
    \begin{enumerate}
        \item Both the loss functions are identical i.e., ${\cal L}_1 = {\cal L}_2$
        \item The optima of the first objective ${\bf w}_1^*$ is same as the optima of ${\cal L}_2$, i.e., ${\bf w}_2^*$
        \item At the optima,  value of the losses are same. i.e., ${\cal L}_1^* = {\cal L}_2^*$
        \item ${\cal L}_1$ is a scalar and ${\cal L}_2$ is a vector
        \item none of the above
    \end{enumerate}
    % Ans: ABC



      \item (use notations  and conventions from the class) Consider the problem of linear regression where we
    minimize the loss
    \[{\cal L}_1 = \frac{1}{N}\sum_{i=1}^N \alpha_i (y_i - {\bf w}^T{\bf x}_i)^2 + \lambda_1 g({\bf w})\] where $g()$ is a regularization term. We also write the loss in matrix form as \[ {\cal L}_2 = \frac{1}{N} [Y-{\bf X}{\bf w}]^TA[Y-{\bf X}{\bf w}] + \lambda_2 g({\bf w}). \]

    \hrule

    If ${\bf A} = I$, $\alpha_i = 2$ for all $i$, and $\lambda_1 = \lambda_2 = 0$, then
    \begin{enumerate}
        \item Both the loss functions are identical i.e., ${\cal L}_1 = {\cal L}_2$
        \item The optima of the first objective ${\bf w}_1^*$ is same as the optima of ${\cal L}_2$, i.e., ${\bf w}_2^*$
        \item At the optima, value of the losses are same. ${\cal L}_1^* = {\cal L}_2^*$
        \item ${\cal L}_1$ is a scalar and ${\cal L}_2$ is a vector
        \item none of the above
    \end{enumerate}
    % Ans: B
    % L_1 = 2 * L_2 but the optimal weights remain same



      \item (use notations  and conventions from the class) Consider the problem of linear regression where we
    minimize the loss
    \[{\cal L}_1 = \frac{1}{N}\sum_{i=1}^N \alpha_i (y_i - {\bf w}^T{\bf x}_i)^2 + \lambda_1 g({\bf w})\] where $g()$ is a regularization term. We also write the loss in matrix form as \[ {\cal L}_2 = \frac{1}{N} [Y-{\bf X}{\bf w}]^TA[Y-{\bf X}{\bf w}] + \lambda_2 g({\bf w}). \]

    \hrule

    If ${\bf A} = I$, $\alpha_i = 1$ for all $i$, and $\lambda_1 \neq \lambda_2 \neq 0$, then
    \begin{enumerate}
        \item The optimal parameters ${\bf w}^*$ is independent of $\lambda_i$.
        \item The larger the lambda, the better the solution.
        \item The smaller the lambda, the better the
        solution
        \item When lambda is nonzero (positive), loss will increase (since $g(w)$ is also positive in practice), better to use $\lambda=0$.
        \item None of the above.
    \end{enumerate}
    % Ans: E
    % Ideally you would tune lambda to see what works best for given data

    \item (use notations  and conventions from the class) Consider the problem of linear regression where we
    minimize the loss
    \[{\cal L}_1 = \frac{1}{N}\sum_{i=1}^N \alpha_i (y_i - {\bf w}^T{\bf x}_i)^2 + \lambda_1 g({\bf w})\] where $g()$ is a regularization term. We also write the loss in matrix form as \[ {\cal L}_2 = \frac{1}{N} [Y-{\bf X}{\bf w}]^TA[Y-{\bf X}{\bf w}] + \lambda_2 g({\bf w}). \]

    \hrule
    See ${\cal L}_2$ closely,
    \begin{enumerate}
        \item When $A$ is a diagonal matrix, this is equivalent to weighing each sample independently.
        \item When $A$ is not a  diagonal matrix, this loss does not make any sense. Don't use.
        \item When $A$ is PD, we can do cholesky decomposition of $A$ as $LL^T$ and an equivalent formulation is possible in ${\cal L}_1$ is each sample getting transformed as ${\bf L}^T{\bf x}_i$ (as in LMNN/Metric Learning)
        \item When $A$ is a rank deficient matrix, an equivalent formulatiion is possible in ${\cal L}_1$ with a dimensionality reduction (this could be proved with eigen decomposition).
        \item None of the above
    \end{enumerate}
    % Ans: ACD


    \item Consider a vocabulary of size $d$. One hot representation of a word $i$ is ``1'' at the location
    (index) corresponding to that word and zero else where.

    Given a document that contains $P$ words,  ${\bf w_1, \ldots, w_P}$, we compute
    \[ {\bf x} = \sum_{i=1}^P {\bf w}_i \]
    Then,
    \begin{enumerate}
        \item ${\bf x}$ is the histogram of the words, with $x_i$ as the frequency of $i$ th word.
        \item ${\bf x}$ is in $R^d$ independent of the number of words in the document.
        \item ${\bf x}$ is in $R^P$ independent of the vocabulary size.
        \item $\sum_{i} {x}_i$ is $P$ ($x_i$ is the $i$ th element of ${\bf x}$)
    \end{enumerate}
    %  Ans: ABD

    \item Consider a document is represented by a histogram of the words in the document. ${\bf h}$ i.e., $h_i$ is the
    number of occurrence of the $i$ th word in the document.

    We define a linguistic operation: Paraphrasing (P1). P1 is defined as permuting sentences in a document and rewriting a sentence by permuting the words.

    \begin{enumerate}
        \item ${\bf h}$ is invariant to the P1
        \item ${\bf h}$ is not invariant to the P1
        \item ${\bf h}$ is invariant under in which order the vocabulary is constructed (eg. "a to z" or "z to a"
        \item a Euclidean distance computed over ${\bf h}_i$ and ${\bf h}_j$ is invariant under in which order the vocabulary is constructed (eg. "a to z" or "z to a".
    \end{enumerate}
    % Ans: AD

    \item Consider a document is represented by a histogram of the words in the document ${\bf h}$ i.e., $h_i$ is the
    number of occurrence of the $i$ th word in the document.

    We define a linguistic operation: Paraphrasing (P2). P2 is defined as replacing a set of words by their synonyms.

    \begin{enumerate}
        \item ${\bf h}$ is invariant to the P2
        \item ${\bf h}$ is not invariant to the P2
        \item ${\bf h}$ is invariant under in which order the vocabulary is constructed (eg. "a to z" or "z to a"
        \item a Euclidean distance computed over ${\bf h}_i$ and ${\bf h}_j$ is invariant under in which order the vocabulary is constructed (eg. "a to z" or "z to a"
    \end{enumerate}
    % Ans: B

    \item A professor suspected that students while submiting home works are doing the paraphasing operations i.e., both P1 and P2. This resulted in failure of  some similarity tests.

    Professor designs a $d\times d$  word similarity matrix ${\bf S}$ such that ${\bf S}_{ij} = {\bf S}_{ji} = 1$ if words $i$ and $j$ are synonyms and zero else. (Note: $d$ is the size of vocabulary).

    Now to compare two documents, professor multiplies the histogram representations by ${\bf S}$.
    \[ {\bf h}_i^\prime = {\bf S}{\bf h}_i  \](Note: ${\bf h}_i^\prime$ is the new representation. Also, note, after multiplying with the ${\bf S}$, the dimension does not change)

    \begin{enumerate}
        \item the new representation is invariant under the operation$P1$ and  $P2$. (i.e.,  All the plagiarism now will be detected.)
        \item the new representation is not invariant for $P2$ and it does not help.
        \item the new representation helps for detecting people who have paraphrased with P2.  But now it
        fails for the documents that were not paraphrased (like the original ones/sincere students!).
        \item the idea is worth, but then ${\bf S}$ should not have made symmetric. with only one of ${\bf S}_{ij}$ or ${\bf S}_{ji}$ as 1. The method could have worked as expected.
    \end{enumerate}
    % Ans: A
    % Diagonal elements of S are 1 since every word is a synonym of itself. Hence i^th element in new representation will be the frequency of i^th word + sum of the frequencies of all synonyms of i^th word

    \item We want to compare two documents $i$ and $j$ which are represented as histogram (popular known as bag of words) of words $h_i$ and $h_j$.

    Here is what four students argued:

    \begin{enumerate}
        \item histograms should be normalized by dividing by the number of words in the document so that the comparison operation becomes ``some what invariant'' to another linguistic operation: "summarization".
        \item Cosine distance is a popular distance to compare two documents using this  representation.
        \item we should remove the stop words (common words in the language) from the sentence so that the comparison will be more useful. Two documents have the same number of `the' does not mean any useful similarity between them.
    \end{enumerate}
    % Ans: ABC


    \item If $A=UDV^T$, then $A^TA$ is
    \begin{enumerate}
        \item $VD^2V^T$
        \item $UD^2U^T$
        \item A square matrix
        \item is always full rank
        \item none of the above
    \end{enumerate}

    \item Consider a set of general vectors ${\bf a}_i\in R^d$. (assume all elements are some random numbers in the range of $[0,1]$) ${\bf b}$ is another such vector. Consider the matrix:
    \[ {\bf A} = \sum_{i=1}^k {\bf a}_i {\bf a}_i^T + \sum_{i=k+1}^d {\bf b}{\bf b}^T\]
    What is the effective rank of ${\bf A}$
    \begin{enumerate}
        \item $k$
        \item $k+1$
        \item $d$
        \item $1$
        \item none of the above
    \end{enumerate}
     \item Consider a set of general vectors ${\bf a}_i\in R^d$. (assume all elements are some random numbers in the range of $[0,1]$) ${\bf b}$ is another such vector. Consider the matrix:

    \[ {\bf A} = \sum_{i=1}^k 10^{-k} {\bf a}_i {\bf a}_i^T + \sum_{i=k+1}^d {\bf b}{\bf b}^T\]
    What is the effective rank of ${\bf A}$
    \begin{enumerate}
        \item $k$
        \item $k+1$
        \item $d$
        \item $1$
        \item none of the above
    \end{enumerate}
     \item Consider a set of general vectors ${\bf a}_i\in R^d$. (assume all elements are some random numbers in the range of $[0,1]$) ${\bf b}$ is another such vector. Consider the matrix:

    \[ {\bf A} = \sum_{i=1}^k 10^{i} {\bf a}_i {\bf a}_i^T + \sum_{i=k+1}^d 10^{-i}{\bf b}{\bf b}^T\]
    What is the effective rank of ${\bf A}$
    \begin{enumerate}
        \item $k$
        \item $k+1$
        \item $d$
        \item $1$
        \item none of the above
    \end{enumerate}
     \item Consider a set of general vectors ${\bf a}_i\in R^d$. (assume all elements are some random numbers in the range of $[0,1]$) ${\bf b}$ is another such vector. Consider the matrix:

    \[ {\bf A} = \sum_{i=1}^k 10^{-i} {\bf a}_i {\bf a}_i^T + \sum_{i=k+1}^d 10^{i}{\bf b}{\bf b}^T\]
    What is the effective rank of ${\bf A}$
    \begin{enumerate}
        \item $k$
        \item $k+1$
        \item $d$
        \item $1$
        \item none of the above
    \end{enumerate}

    \item Consider a matrix $A$ of size $m\times n$. Rank of $A$ is (choose one one most correct answer)
\begin{enumerate}
\item $\leq \min(m,n)$
\item $\leq \max (m,n)$
\item $\geq \min(m,n)$
\item $\geq \max(m,n)$
\item $\frac{m+n}{2}$
\end{enumerate}

\item A and B are two independent events such that $P(\overline A) = 0.4$ and $P(A \cap B) = 0.2$ Then $P(A \cap \overline B)$ is equal to
    \begin{enumerate}
        \item 0.4
        \item 0.2
        \item 0.6
        \item 0.8
        \item None of  the above
    \end{enumerate}
    % Ans: A

\item If ${\bf A}$ is a $n\times n$ matrix, with every pair of columns orthogonal i.e., ${\bf a_i\cdot a_j = 0}\,\,\, \forall i,j$ and $||{\bf a_i}|| = 1$. Then:
\begin{enumerate}
    \item ${\bf A^{-1}} = {\bf A}^T$.
    \item ${\bf A}{\bf A}^T = {\bf I}$
    \item ${\bf A}{\bf A}^T$ has only one 1 in every column and all others zero.
    \item ${\bf A}^{-1}$ has only one 1 in every column and all others zero.
    \item none of the above
\end{enumerate}
\item Product of Eigen values of a real square matrix is:
\begin{enumerate}
    \item Determinant
    \item Rank
    \item Trace
    \item non-Negative
    \item None of the above
\end{enumerate}

\item $X\sim N(0,1)$, $Y\sim N(1,1)$ and $Z=X+Y$. Then,
    \begin{enumerate}
        \item $Z\sim N(0,2)$
        \item $Z\sim N(0,1)$
        \item $Z\sim N(1,1)$
        \item $Z\sim N(1,2)$
        \item None of the above
    \end{enumerate}
    % Ans: D

\end{enumerate}











\section{Part 2}
\begin{enumerate}
    \item In the video titled "MSE as MLE", at 0:56, we are computing a quantity $\epsilon_i$. This quantity is
        \begin{enumerate}
            \item The perpendicular distance of the points from the line
            \item The distance of the points from the line along X-axis
            \item The distance of the points from the line along Y-axis
            \item None of these
        \end{enumerate}
    [Refer \url{https://youtu.be/XfmZ3Ap-xHI?t=56}]
    % Ans: D


    \item In the video titled "Data from Multivariate Gaussians", at 06:39, we are talking about isocontours. Consider for a 2-D gaussian, the isocontours are circular. Suppose we have two such circles $c_1, c_2$, with radii $r_1$ and $r_2$ respectively. The probabilities of each point on $c_1$ is $p_1$ and that on $c_2$ is $p_2$. Then, given that $r_1>r_2$, we can say that
    \begin{enumerate}
        \item $p_1<p_2$
        \item $p_1>p_2$
        \item $p_1=p_2$
        \item Cant be said from given information
    \end{enumerate}
    [Refer \url{https://youtu.be/AsQORY4_H9Q?t=399}]
    % Ans: B


    \item In the video titled "Bias and variance", we talk about "underfitting" and "overfitting". Suppose we have are trying to fit a polynomial on a given data and we are overfitting in our problem. Which of these could be a possible solution
    \begin{enumerate}
        \item Collect more data
        \item Increase the degree of the polynomial
        \item Decrease the degree of the polynomial
        \item None of these
    \end{enumerate}
    [Refer \url{https://youtu.be/IqdY9NWdhLk?t=274}]
    % Ans: AC

    \item In the video titled "Decision Boundaries for Multivariate Gaussians", at 2:09, we are talking of a closed form expression for $\theta$.

    Given $(X\vert \omega_1)\sim N(\mu_1,\sigma_1)$ and $(X\vert \omega_2)\sim N(\mu_2,\sigma_2)$ Derive the expression for $\theta$ in a 1-dimensional scenario
    \begin{enumerate}
        \item $\theta$ is the solution of a linear equation
        \item $\theta$ is the solution of a quadratic equation
        \item $\theta$ is the solution of an exponential equation
        \item $\theta$ cannot be calculated in closed form
    \end{enumerate}
    [Refer \url{https://youtu.be/Nc1Z41fEYhc?t=129}]
    % Ans: B

    \item In the video titled "Regularization in Regression" we assert that L1 regularization leads to sparsity while L2 does not. We want to prove/disprove this:

    % Consider an experiment to prove or disprove this.
    Consider the vector $x = (1,\epsilon)\in R^2$ where $\epsilon>0$ is small. Suppose that as part of some regularization procedure, we have to reduce one of the elements of $x$ by $\delta$.

    This gives two options: $x_1$: $(1-\delta,\epsilon)$ and $x_2$: $(1,\epsilon-\delta)$
    \begin{enumerate}
        \item L1 norm of $x_1$ and $x_2$ are same
        \item L2 norm of $x_1$ and $x_2$ are same
        \item Decrease in L2 norm from $x$ to $x_1$ is more than that from $x$ to $x_2$
        \item This proves that L1 norm promotes decrease of the larger quantity in $x$ rather than reducing the smaller one to zero
        \item This proves that L2 norm promotes decrease of the larger quantity in $x$ rather than reducing the smaller one to zero
    \end{enumerate}
    [Refer \url{https://youtu.be/DnRWPq1UGyA?t=180}]
    % Ans: ACE

\end{enumerate}










\section{Part 3}
\begin{enumerate}
    % The values in these questions have to be generated by code
    % Different values of m: integer(2,6), and p: float(.1,.9) produce different questions
    \item In  a MCQ a student randomly guesses from the options if she does not know. Given that there were $m$ choices in a question and that $p$ is the chance she knows the answer, what is the probability that she knew the answer if she answered correctly?
    \begin{enumerate}
        \item $ \frac {mp} {1+mp} $
        \item $ \frac {1} {1+(m-1)p} $
        \item $ \frac {1} {1+mp} $
        \item $ \frac {mp} {1+(m-1)p} $
    \end{enumerate}
    %Ans: D

    % Different values of w1, b1, w2, b2 produce different answers. (All integers in the range (1,10)
    \item Bag I contain $w_1$ white and $b_1$ black balls. Bag II contains $w_2$ white and $b_2$ black balls.

    A ball is drawn at random from one of the bags, and it is found to be white. What is the probability that it was drawn from Bag I.
    \begin{enumerate}
        \item $\frac{w_1}{w_1 + b_1}$
        \item $\frac{w_1(w_1+b_1)}{w_1(w_1+b_1) + w_2(w_2+b_2)}$
        \item $\frac{w_1(b_1+b_2)}{w_1(b_1+b_2) + w_2(b_1+b_2)}$
        \item $\frac{w_1(w_2+b_2)}{w_1(w_2+b_2) + w_2(w_1+b d_1)}$
    \end{enumerate}
    % Ans: D

    % Different values of K: int[0,10] produces different questions
    \item A man is known to speak truth $K$ out of 10 times. He throws a die and reports that number obtained is a four. Find the probability that the number obtained is actually a four.
    \begin{enumerate}
        \item $\frac 1 6$
        \item $\frac {K}{60-5K} $
        \item $\frac {K}{40-3K} $
        \item $\frac {K}{50-4K} $
    \end{enumerate}
    % Ans: D

    % Different values of A,B,C,D can be used to produce questions
    \item Given the following confusion matrix what is the precision?
    \begin{table}[!htbp]\centering\begin{tabular}{|c|c|c|}
    \hline
               & Predicted +ve & Predicted -ve \\ \hline
    Actual +ve & A             & B             \\ \hline
    Actual -ve & C             & D             \\ \hline
    \end{tabular}\end{table}
    \begin{enumerate}
        \item $\frac{A}{A+B}$
        \item $\frac{A}{A+C}$
        \item $\frac{A+D}{A+B+C+D}$
        \item $\frac{D}{D+C}$
    \end{enumerate}
    % Ans: B

    % Different values of N (integers in range 3 to 10)
    \item Consider that numbers from 1 to $N^2$ are arranged in a $N$ dimensional square matrix $M$. The rank of $M$ is
    \begin{enumerate}
        \item 1
        \item 2
        \item $N$
        \item None of these
    \end{enumerate}


\end{enumerate}









\section{Part 4}
\begin{enumerate}
    \item We know that solution to the problem of Maximize ${\bf w}^T{\bf A}{\bf w}$ subject to $||{\bf w}|| = 1$ is the eigen vector corresponding to the largest eigen value.

    Note that we assume that a typical eigen value computation assumes to be returning (i) eigen values arranged in non-increasing order (ii) eigen vectors have unit L2 norm.

    What is the solution to the problem of
     Maximize ${\bf w}^T{\bf A}{\bf w}$ subject to $||{\bf w}||^2_2 = 2$

     \begin{enumerate}
         \item Eigen vector correspond to the second  eigen value.
         \item Eigen vector correspond to the first eigen value.
         \item 2*{\bf u} where ${\bf u}$ is the eigen vector correspond to the first eigen value.
         \item $\sqrt{2}*{\bf u}$ where ${\bf u}$ is the eigen vector correspond to the first eigen value.
         \item None of the above.
     \end{enumerate}
     % Ans: D


     \item We are working with $N$ samples each of $d$ dimension.
     Consider $N< d$

     \begin{enumerate}
         \item Solution to the problem of linear regression as a closed form can not be computed because the matrices are no longer compatble for multiplication.

         \item Solution to the problem of linear regression as a closed form can not be computed because the matrix can not be inverted.

         \item Solution to the problem of ridge regularized linear regression as a closed form can not be computed because the matrix can not be inverted.

         \item None of the above
     \end{enumerate}
     % Ans: B (Covariance matrix becomes non full-rank, hence singular). But on adding lambda I, it can become full rank



     \item We are working with $N$ samples each of $d$ dimension.
     Consider $N\le d$

     \begin{enumerate}
         \item PCA can not be computed
         \item While computing Eigen values, we will see $d$ zero eigen values.
         \item While computing Eigen values, we will see at least $d-N$ zero eigen values.
         \item While computing Eigen values, we will see at max $d-N$ zero eigen values.
         \item We can not use eigen value/vector computation. We need to use SVD.
         \item None of the above.
     \end{enumerate}
     % BCD
     % Rank of covariance matrix will be N so that many non-zero eigen values. Rest of d-N eigenvalues are 0



     \item We know the weighted Euclidean distance
     \[ \tau = [{\bf x} - {\bf y}]^T[{\bf A}][{\bf x} - {\bf y}] \]

     Where {\bf x} is a vector in $d$ dimension ${\bf A}$ is a square matrix

     \begin{enumerate}
         \item If ${\bf A}$ is a non-identity diagonal matrix. Then ``dist'' is a scaled version of Euclidean distance or ``$\tau  = \alpha$  Euclidean distance''

         \item If $k < d$ and $A = \sum_{i=1}^k {\bf u}_i{\bf u}_i^T$, with ${\bf u}_i$s as orthonormal. Then $A$ is rank deficient and Euclidean $\tau$ can not be computed.

         \item If $k < d$ and $A = \sum_{i=1}^k {\bf u}_i{\bf u}_i^T$, with ${\bf u}_i$s as orthonormal. Then $\tau$ is equivalent to dimensioality reduction ${\bf x}^\prime = {\bf W}{\bf x}$ with ${\bf W}$ as $k\times d$ matrix with ${\bf u}_i^T$ as the $i$ th row.

         \item When ${\bf A}$ is non-diagonal matrix, $\tau$ can not be a metric.

         \item None of the above.

     \end{enumerate}
     % Ans: AC

     \item Consider the covariance matrix $\Sigma$

     \begin{enumerate}
         \item $\Sigma$ is symmetric
         \item $\Sigma$ is PSD
         \item $\Sigma$ is Diagonal if the distribution is Normal.
         \item $\Sigma$ can not be Diagonal if the distribution is Normal.
         \item None of the above
     \end{enumerate}
     % Ans: AB


\end{enumerate}
\end{document}
